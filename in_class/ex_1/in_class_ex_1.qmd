---
title: "In Class ex 1 - Chap 3 Spatial Weights and Applications"
editor: visual
---

# Overview

# Step 1: Check and load packages

In this chapter, we will learn how to compute spatial weights using R.

Check if we have required packages:

```{r}
packages = c('sf', 'spdep', 'tmap', 'tidyverse')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}
```

Import the required packages:

```{r}
pacman::p_load(sf,spdep,tmap,tidyverse)
```

# Step 2: Loading the data into R

We will first load the shapefile into the environment:

```{r}
hunan <- st_read(dsn = "dataH/geospatial", layer = "Hunan")
```

We will then check the loaded file:

```{r}
list(hunan)
```

Next we will Import the CSV file:

```{r}
hunan2012 <- read_csv("dataH/aspatial/Hunan_2012.csv")
```

After the CSV files is imported, we will check it:

```{r}
list(hunan2012)
```

After the CSV file is imported, we need to perform a relational join using **dplyr**:

\*\*Note: Here it seems relational join magically detects the common feature to join by. WOW !

```{r}
hunan <- left_join(hunan,hunan2012)
```

# Step 3: Visualize using plots

Next we will make use of the **tmap** package (tm_shape, tm_text, qtm):

```{r}
basemap <- tm_shape(hunan) +
  tm_polygons() +
  tm_text("NAME_3", size=0.2)

gdppc <- qtm(hunan, "GDPPC")
tmap_arrange(basemap, gdppc, asp=1, ncol=2)
```

# Step 4: Computing the Contiguity Spatial Weights

## Queen:

Compute the (QUEEN) contiguity based neighbors weight matrix:

```{r}
wm_q <- poly2nb(hunan, queen = TRUE)
summary(wm_q)
```

Next we want to see the neighbor for the first polygon:\
\*\*Note: Both syntax below seem to work

```{r}
wm_q[1]
wm_q[[1]]
```

Next we will retrieve the name of polygon 1:

```{r}
hunan$County[1]
```

We will next reveal the neighbouring counties of Anxiang:

```{r}
hunan$NAME_3[c(2,3,4,57,85)]
```

We can also retrieve the neighbouring GDPPC of the five countries by code chunk below:

\*\*Note: we can save the neighbor as a list.

\*\*Then we will just use that as the input for filtering

```{r}
nb_1 <- wm_q[[1]]
nb_1 <- hunan$GDPPC[nb_1]
nb_1
```

Using str(), we will then display the complete weight matrix:

```{r}
str(wm_q)
```

## Rook:

Create the weight matrix based on the ROOK:

```{r}
wm_r <- poly2nb(hunan, queen = FALSE)
summary(wm_r)
```

# Step 5: Visualize the contiguity weights:

## Data preparation:

Store the longitude:

\*\*\~ means shorthand for writing a function

\*\* .x means a vector input

```{r}
longitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])
```

Store the latitude:

```{r}
latitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])
```

Bind the 2 above together:

```{r}
coords <- cbind(longitude, latitude)
```

Check the head of the newly created data:

```{r}
head(coords)
```

## Plot QUEEN:

Next we will plot the contiguity based neighbors map:

```{r}
plot(hunan$geometry, border = "lightgrey")
plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= "red")
```

## Plot ROOK:

```{r}
plot(hunan$geometry, border="lightgrey")
plot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = "red")
```

Plotting both maps together:

```{r}
par(mfrow=c(1,2))
plot(hunan$geometry, border="lightgrey")
plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= "red", main="Queen Contiguity")
plot(hunan$geometry, border="lightgrey")
plot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = "red", main="Rook Contiguity")
```

# Step 6: Compute distance based neighbors:

## Compute the neighbors:

Calculate the nearest neighbor:

\*\*k is default value 1

```{r}
k1 <- knn2nb(knearneigh(coords))
```

Here we see that each polygon only has 1 neighbor:

```{r}
str(k1)
```

compute the distance between all neighbors:

```{r}
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)
```

Summary report shows that the max value is 61.79, so this value will be used as the upper threshold

## Compute the weight matrix:

We compute the distance weight matrix as follow:

\*\*0 is min threshold

\*\*62 is max threshold

```{r}
wm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)
wm_d62
```

\*\*Average number of links here refers to the average number of nearest neighbor per polygon

Display the weight matrix:

```{r}
str(wm_d62)
```

An alternative way to present the data:

```{r}
table(hunan$County, card(wm_d62))
```

Find number of disjoint sub graph:

<https://r4gdsa.netlify.app/chap03.html#computing-distance-based-neighbours>

```{r}
n_comp <- n.comp.nb(wm_d62)
n_comp$nc
```

```{r}
table(n_comp$comp.id)
```

## Plot the distance based weight matrix:

Plot the distance based neighbor + K1 nearest neighbor:

\*\*Note it is an overlap of 2 plots on 1

```{r}
plot(hunan$geometry, border="lightgrey")
plot(wm_d62, coords, add=TRUE)
plot(k1, coords, add=TRUE, col="red", length=0.08)
```

Plot the distance based + K nearest neighbor on 2 separate plots:

```{r}
par(mfrow=c(1,2))
plot(hunan$geometry, border="lightgrey")
plot(k1, coords, add=TRUE, col="red", length=0.08, main="1st nearest neighbours")
plot(hunan$geometry, border="lightgrey")
plot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6, main="Distance link")
```

## Compute adaptive distance weight matrix:

Recalculate the N nearest neighbors, this time using K = 6

```{r}
knn6 <- knn2nb(knearneigh(coords, k=6))
knn6
```

Check the output:

```{r}
str(knn6)
```

Plot the new K = 6:

```{r}
plot(hunan$geometry, border="lightgrey")
plot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = "black")
```

# Step 7: Inversed Distance method

First we will compute the distance between all polygons:

```{r}
dist <- nbdists(wm_q, coords, longlat = TRUE)
```

Apply a simple function:

```{r}
ids <- lapply(dist, function(x) 1/(x))
ids
```

Get the row standardized weight matrix:

\*\*Note , here we use style W

\*\*But style W may not be that robust. We can also use style B

\*\*zero policy is set to true for list of non-neighbors

```{r}
rswm_q <- nb2listw(wm_q, style="W", zero.policy = TRUE)
rswm_q
```

We check the output:

```{r}
rswm_q$weights[10]
```

Next we will compute the row standardized distance weight matrix:

\*\*Note here we will need to use "ids" which is the inversed distance

```{r}
rswm_ids <- nb2listw(wm_q, glist=ids, style="B", zero.policy=TRUE)
rswm_ids
```

Check the output:

```{r}
rswm_ids$weights[1]
```

Check the summary:

```{r}
summary(unlist(rswm_ids$weights))
```

# Step 8: Apply spatial weight matrix:

## Spatial lag with row-standardized weights:

Compute the spatial lag:

```{r}
GDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)
```

The lag for the first polygon is:

```{r}
GDPPC.lag[1]
```

The GDPPC of the first polygon's neighbor are:

```{r}
nb1 <- wm_q[[1]]
nb1 <- hunan$GDPPC[nb1]
nb1
```

Their total is:

```{r}
sum(nb1)
```

Their average is:

```{r}
sum(nb1)/5
```

So their weighted averaged is the value of the lag of the first polygon.

Append the values:

```{r}
lag.list <- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))
lag.res <- as.data.frame(lag.list)
colnames(lag.res) <- c("NAME_3", "lag GDPPC")
hunan <- left_join(hunan,lag.res)
```

```{r}
head(hunan)
```

Plot the lag GDPPPC:

```{r}
gdppc <- qtm(hunan, "GDPPC")
lag_gdppc <- qtm(hunan, "lag GDPPC")
tmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)
```

## Spatial lag as sum of neighbouring values:

Calculate the weights:

```{r}
b_weights <- lapply(wm_q, function(x) 0*x + 1)
b_weights2 <- nb2listw(wm_q, 
                       glist = b_weights, 
                       style = "B")
b_weights2
```

Compute lag variable:

```{r}
lag_sum <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))
lag.res <- as.data.frame(lag_sum)
colnames(lag.res) <- c("NAME_3", "lag_sum GDPPC")
```

Examine the output:

```{r}
lag_sum
```

Join the data:

```{r}
hunan <- left_join(hunan, lag.res)
```

Plot the lag sum:

```{r}
gdppc <- qtm(hunan, "GDPPC")
lag_sum_gdppc <- qtm(hunan, "lag_sum GDPPC")
tmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)
```

## Spatial window average:

Assign to new variable:

```{r}
wm_q1 <- wm_q
```

Next we will use include.self() to set the attribute to include self in neighbor calculation.

```{r}
include.self(wm_q1)
```

Obtain the weights:

```{r}
wm_q1 <- nb2listw(wm_q1)
wm_q1
```

Create lag variable:

```{r}
lag_w_avg_gpdpc <- lag.listw(wm_q1, 
                             hunan$GDPPC)
lag_w_avg_gpdpc
```

Convert to dataframe:

```{r}
lag.list.wm_q1 <- list(hunan$NAME_3, lag.listw(wm_q1, hunan$GDPPC))
lag_wm_q1.res <- as.data.frame(lag.list.wm_q1)
colnames(lag_wm_q1.res) <- c("NAME_3", "lag_window_avg GDPPC")
```

Join to mainframe:

```{r}
hunan <- left_join(hunan, lag_wm_q1.res)
```

Plot the outcome:

```{r}
gdppc <- qtm(hunan, "GDPPC")
w_avg_gdppc <- qtm(hunan, "lag_window_avg GDPPC")
tmap_arrange(gdppc, w_avg_gdppc, asp=1, ncol=2)
```

## Spatial Window Sum:

We will not proceed without row standardized weights:

Copy new variable

```{r}
wm_q1 <- wm_q
```

Set attribute:

```{r}
include.self(wm_q1)
```

```{r}
wm_q1
```

Apply function:

```{r}
b_weights <- lapply(wm_q1, function(x) 0*x + 1)
b_weights[1]
```

Get weight values:

```{r}
b_weights2 <- nb2listw(wm_q1, 
                       glist = b_weights, 
                       style = "B")
b_weights2
```

Compute lag variable:

```{r}
w_sum_gdppc <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))
w_sum_gdppc
```

Convert result into dataframe:

```{r}
w_sum_gdppc.res <- as.data.frame(w_sum_gdppc)
colnames(w_sum_gdppc.res) <- c("NAME_3", "w_sum GDPPC")
```

Join to main frame:

```{r}
hunan <- left_join(hunan, w_sum_gdppc.res)
```

Plot the results:

```{r}
gdppc <- qtm(hunan, "GDPPC")
w_sum_gdppc <- qtm(hunan, "w_sum GDPPC")
tmap_arrange(gdppc, w_sum_gdppc, asp=1, ncol=2)
```
