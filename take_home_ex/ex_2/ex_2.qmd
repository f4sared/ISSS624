---
title: "Take Home Ex 2"
editor: visual
---

# Overview

[**Files required for Take Home Ex 2:**]{.underline}

Water point attribute data (exported as CSV file):\
<https://www.waterpointdata.org/access-data/>\
Click on the button "Access WPdx+ Global Data Repository"\
Then click export button & download as CSV file

Nigeria Administrative Level 2 Boundary File:\
<https://www.geoboundaries.org/>\
Enter "Nigeria" in the name filter column.\
Download Nigeria-NGA-ADM2-2022 under the column "geoBoundaries"

Column Description can be found at:\
<https://www.waterpointdata.org/wp-content/uploads/2021/04/WPDx_Data_Standard.pdf>

# Step 1: Importing the required packages

The following packages are imported:

-   here: We use this to generate a path to the file stored at the root directory

-   sf: We use this for manipulation of simple features

-   funModeling:

-   tmap: We use for thematic plotting

-   tidyverse (<https://www.tidyverse.org/>)

    -   ggplot2

    -   dplyr: Used for data manipulation (e.g. mutate() )

    -   readr: We use this for reading rectangular data like csv

-   spdep:

-   corrplot: For plotting of the correlation object

-   heatmaply: For plotting of heatmaps and also for normalize() function

-   cluster:

-   factoextra: Extract and Visualize the Results of Multivariate Data Analyses

```{r}
pacman::p_load(here,
               sf, tidyverse, spdep,
               funModeling, tmap,
               corrplot, heatmaply, cluster, factoextra
               )
```

# Step 2: Shrinking the CSV file

Since the CSV file is really huge, we need to shrink it down first so we can store a copy of it on github. To do this, we will import the CSV file and then only select the rows that we are keen to keep using the select() function. The CSV will initially be imported as a tibble file format, thereafter once we select our referred columns, we also need to convert geometry coordinate before finally saving it as a rds file.

Using the here() function, we will generate a path to where the large csv file is stored:

```{r}
#| eval: true
csv_path <- here("data", "dataNigeria_2", "waterpoint", "wpdx.csv")
csv_path
```

We will use the read_csv() of the readr package to ingest the CSV file:

```{r}
#| eval: false
wp_nga <- read_csv(csv_path) %>% filter(`#clean_country_name` == "Nigeria")
```

We will use the select() to keep only columns of interest:

```{r}
#| eval: false
wp_nga_out <- wp_nga %>% select(7:10, 13:14, 22, 46:47, 62)
```

Using st_as_sfc() of the sf package, we derive a new "Geometry" column based on \`New Georeferenced Column\`. This is because the column is as actually holding data in textual format.

We run the code as follow:

```{r}
#| eval: false
wp_nga_out$Geometry = st_as_sfc(wp_nga_out$`New Georeferenced Column`)
```

Next using st_sf() of the sf package, we will convert the current object to sf dataframe:

```{r}
#| eval: false
wp_nga_out <- st_sf(wp_nga_out, crs=4326) 
```

Since the column \`New Georeferenced Column\` is no longer needed, we exclude it:

```{r}
#| eval: false
wp_nga_out <- wp_nga_out %>% select(1:9, 11)
```

To save the file, we need to generate a save path:

```{r}
#| eval: true
savefile_path <- here("data", "dataNigeria_2", "waterpoint", "wp_nga_v2.rds")
savefile_path
```

We then save the dataframe as a rds file:

```{r}
#| eval: false
write_rds(wp_nga_out, savefile_path)
```

# Step 3: Loading the data

## Load the water point data:

Now, using the read_rds of the readr package, we load the data as follow:\
(we will rename all the columns for ease of work)\
(we will also replace all N/A values of the status with "Unknown")

```{r}
wp_nga <- read_rds(savefile_path) %>%
    rename(`status`=`#status_clean`) %>%
    rename(`adm1`=`#clean_adm1`) %>%
    rename(`adm2`=`#clean_adm2`) %>% 
    rename(`tech_clean`=`#water_tech_clean`) %>%
    rename(`tech_category`=`#water_tech_category`) %>%
    mutate(status = replace_na(status, "Unknown")) %>%
    mutate(tech_category = replace_na(tech_category, "Unknown")) %>% 
    mutate(tech_clean = replace_na(tech_clean, "Unknown"))
```

We will now use the st_geometry of the sf package to look at our data info:

```{r}
st_geometry(wp_nga)
```

## Load the Geo Boundary data:

Once again, we need to generate the path as follow:

```{r}
boundary_path <- here("data", "dataNigeria_2", "boundary")
boundary_path
```

Using the st_read() function of the sf package, we will load the boundary file:

```{r}
nga <- st_read(dsn = boundary_path,
               layer = "geoBoundaries-NGA-ADM2",
               crs = 4326) %>% select(shapeName)
```

## Handling Repeated Names

We refer to our classmates' method:\
<https://jordan-isss624-geospatial.netlify.app/posts/geo/geospatial_exercise/#data-wrangling>

Get the index of the rows that are duplicated

```{r}
bool_list <- duplicated(nga$shapeName)
duplicated_names <- nga$shapeName[bool_list]
index_rows <- which(nga$shapeName %in% duplicated_names)
index_rows
```

Select rows that are duplicated

```{r}
dup_rows <- nga %>% filter(nga$shapeName %in% duplicated_names)
dup_rows$shapeName
```

Plot the areas that are duplicated:

```{r}
#| eval: false
tmap_mode("view")
tm_shape(dup_rows) + tm_polygons()
```

Rename the duplicated areas

```{r}
nga$shapeName[index_rows] <- c("Bassa_1", "Bassa_2", 
                               "Ifelodun_1", "Ifelodun_2",
                               "Irepodun_1", "Irepodun_2",
                               "Nasarawa_1", "Nasarawa_2",
                               "Obi_1", "Obi_2",
                               "Surulere_1","Surulere_2")
```

Check to see if there are any repeated rows again:

```{r}
bool_list <- duplicated(nga$shapeName)
duplicated_names <- nga$shapeName[bool_list]
index_rows <- which(nga$shapeName %in% duplicated_names)
index_rows
```

# Step 4: Data Wrangling

## Status of water point

Using the freq() from funModeling, we will check the distribution of status of the water points:

```{r}
freq(data=wp_nga, input = 'status')
```

### Functional water point

```{r}
wpt_functional <- wp_nga %>%
  filter(status %in%
           c("Functional",
             "Functional but not in use",
             "Functional but needs repair"))
```

```{r}
#| eval: false
freq(data=wpt_functional, 
     input = 'status')
```

### Non Functional water point

```{r}
wpt_nonfunctional <- wp_nga %>%
  filter(status %in%
           c("Abandoned/Decommissioned", 
             "Abandoned",
             "Non-Functional",
             "Non functional due to dry season",
             "Non-Functional due to dry season"))
```

```{r}
#| eval: false
freq(data=wpt_nonfunctional, 
     input = 'status')
```

### Unknown

```{r}
wpt_unknown <- wp_nga %>% filter(status == "Unknown")
```

## Water point Technology

```{r}
wpt_hand_pump <- wp_nga %>% filter(tech_category == "Hand Pump")
```

```{r}
#| eval: false
freq(data=wp_nga, input = 'tech_category')
```

## Usage Capacity

```{r}
wpt_1000 <- wp_nga %>% filter(usage_capacity < 1000)
```

```{r}
#| eval: false
freq(data=wp_nga, input = 'usage_capacity')
```

## Rural waterpoints

```{r}
wpt_rural <- wp_nga %>% filter(is_urban == FALSE)
```

```{r}
#| eval: false
freq(data=wp_nga, input = 'is_urban')
```

## Point-in-Polygon

Point in polygon

```{r}
#| eval: true
nga_wp <- nga %>%
  mutate(`total_wpt` = lengths(
    st_intersects(nga, wp_nga))) %>%
  mutate(`wpt_functional` = lengths(
    st_intersects(nga, wpt_functional))) %>%
  mutate(`wpt_non_functional` = lengths(
    st_intersects(nga, wpt_nonfunctional))) %>%
  mutate(`wpt_unknown` = lengths(
    st_intersects(nga, wpt_unknown))) %>% 
  mutate(`wpt_hand_pump` = lengths(
    st_intersects(nga, wpt_hand_pump))) %>%
  mutate(`wpt_1000` = lengths(
    st_intersects(nga, wpt_1000))) %>% 
  mutate(`wpt_rural` = lengths(
    st_intersects(nga, wpt_rural))) 
```

Calculate Percentage

```{r}
#| eval: true
nga_wp <- nga_wp %>%
  mutate(pct_functional = `wpt_functional`/`total_wpt`) %>%
  mutate(`pct_non_functional` = `wpt_non_functional`/`total_wpt`) %>% 
  mutate(`pct_hand_pump` = `wpt_hand_pump`/`total_wpt`) %>% 
  mutate(`pct_1000` = `wpt_1000`/`total_wpt`) %>%
  mutate(`pct_rural` = `wpt_rural`/`total_wpt`)
```

Replace NaN with 0

```{r}
#| eval: true
NaN_list <- is.nan(nga_wp$pct_functional)
nga_wp$pct_functional[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_non_functional)
nga_wp$pct_non_functional[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_hand_pump)
nga_wp$pct_hand_pump[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_hand_pump)
nga_wp$pct_hand_pump[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_1000)
nga_wp$pct_1000[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_rural)
nga_wp$pct_rural[NaN_list] <- 0
```

```{r}
#| eval: false
qtm(nga_wp, fill = "wpt hand pump") + 
    tm_layout(legend.height = 0.4,legend.width = 0.4)
```

# Step 6: Correlation Analysis

Get correlation variables as follow:\
(we use st_set_geometry(NULL) of sf package to exclude geometric data)

```{r}
corr_data <- nga_wp[,c(4:5, 10:14)] %>% st_set_geometry(NULL)
```

In order to perform correlation analysis, we need to get the correlation matrix using cor() from the base R package, then we can plot it using the corrplot.mixed() function from corrplot package.

Perform correlation analysis:

```{r}
#| eval: true
cluster_vars.cor = cor(corr_data)
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

From the correlation plot above, we notice that the percentage of hand pump is highly negatively correlated with the percentage of well with 1000 capacity. Because the value is not that high extremely high, we always include it in the analysis at another time.

# Step 7: Hierarchy Cluster Analysis

## Extract the clustering variables

To perform the analysis, we need to extract the cluster variables without the geometric data. To do this, we we will use the function st_set_geometry(NULL) from sf package on our sf dataframe.

We run the code as follow:

```{r}
cluster_vars <- nga_wp %>%
  st_set_geometry(NULL) %>%
  select("shapeName", "wpt_functional", "wpt_non_functional", 
         "pct_functional", "pct_non_functional", 
         "pct_hand_pump", "pct_1000", "pct_rural")
head(cluster_vars,3)
```

Next, we need to change the row names using row.name() function from R base:

```{r}
row.names(cluster_vars) <- cluster_vars$"shapeName"
head(cluster_vars,3)
```

Now, we do not need the column shapeName & pct_1000 anymore, so we exclude it:

```{r}
cluster_vars <- select(cluster_vars, c(2:6, 8))
head(cluster_vars, 3)
```

## Data Standardization

From our dataframe "done_cluster_vars", we can see that the columns wpt_functional and wpt_non_functional have different ranges since they are counts instead of percentages. Thus we need to perform standardization on these columns. This is because analysis results will be biased towards clustering variables with large values.

Hence the need for standardization before cluster analysis

We will therefore perform the min-max standardization using the normalize() of the heatmaply package:\
(we use summary() of base R package to check the outcome)

```{r}
std_cluster_vars <- normalize(cluster_vars)
summary(std_cluster_vars)
```

## Compute the proximity matrix

```{r}
proxmat <- dist(std_cluster_vars, method = 'euclidean')
```

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

## Plotting the Dendrogram:

To view the outcome, we save the output as a png and then load it as a picture:\
<https://stackoverflow.com/questions/11444743/how-to-adjust-sizes-of-x-axis-in-dendrogram-r>

Each time the code is run, the .png object will be updated. For better resolution, we increase the width and height of the .png file. The "cex" is adjusted such that the names of each region is just visible when we zoom in on the .png object.

Additional parameters:

-   lwd: Line width

-   main: Title

-   line: Position of title

-   cex.main: Size of title

We run the code as follow

```{r}
png("plotdendogram.png",width=12800,height=3200)
plot(hclust_ward, cex = 1.2, lwd=3.0, main="Dendrogram", line = -20, cex.main=20)
dev.off()
```

Please open the png in a new tab and then zoom in:

![](plotdendogram.png)

## Selecting the optimal clustering algorithm:

In order the measure the strength of the clustering structure, we will need to measure the agglomerative coefficient. If we compare the aforementioned coefficient across various clustering algorithms, we will be able to compare and then select the algorithm that gives us the best clustering structure.

Normally, we can do this via the agnes() function from the cluster package:

```{r}
agnes_result <- agnes(std_cluster_vars, method = "average")
agnes_result$ac
```

The results are stored in the output data structure under the name agnes_result\$ac. The higher the magnitude of this value, the better the clustering structure. We will need to test all 4 of the algorithm to determine which is the best to apply.

First, we need to create a data structure to hold the names of the clustering algorithms:

```{r}
algorithms <- c( "average", "single", "complete", "ward")
names(algorithms) <- c( "average", "single", "complete", "ward")
algorithms
```

Next we write a simple function "test" that takes in the name of the clustering algorithm and then outputs the score of the clustering structure:

```{r}
test <- function(x) {
  agnes(std_cluster_vars, method = x)$ac
}
```

Finally, using the map_dbl() function from the purr package, we will map the name of each algorithm to the input of the functions:

```{r}
map_dbl(algorithms, test)
```

## Determine optimal clusters

In order to perform meaningful analysis, we need to select the optimal amount of cluster. Too little clusters, we won't be able to draw meaningful insights. Too many clusters, and we will also fail to draw meaning.

There are 3 methods to do this:

-   Elbow method

-   Average silhouette method

-   Gap statistic

For this, we will make use of the tool called the gap statistic: \
<https://hastie.su.domains/Papers/gap.pdf>

The gap statistic is a measure of intra-cluster dispersal. The larger this value, the bigger the indication that the clustering structure is far from the random distribution of points. As we increase the number of cluster, we want to get a highest gap statistic number possible. The minimum number of clusters that we accept is 3 cluster for meaningful analysis.

There are a few methods to determine clusters:

-   firstSEmax

-   Tibs2001SEmax

-   globalSEmax

-   firstmax

-   globalmax

Useful links: \
<https://stats.stackexchange.com/questions/95290/how-should-i-interpret-gap-statistic>\
<https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clusGap.html>

To do this, we use the clusGap() function from the cluster package: \
(for predictability, we need to set a specific random seed)

```{r}
set.seed(2022)
gap_stat <- clusGap(std_cluster_vars, 
                    FUN = hcut, 
                    # nstart = 25, 
                    K.max = 25, 
                    B = 50)
# Print the result
print(gap_stat, method = "Tibs2001SEmax")
```

Next, we will plot the Gap Statistics as follow:

```{r}
fviz_gap_stat(gap_stat)
```

From the graph we see that the gradient of the Gap Statistic decreases greatly from the 4th cluster onwards. Therefor, we will opt to keep 4 clusters + 1 more for checking.

## Interpreting the Dendrograms

```{r}
png("plotdendogram_0.png",width=12800,height=3200)
plot(hclust_ward, cex = 1.2, lwd=3.0, main="Dendrogram", line = -20, cex.main=20)
rect.hclust(hclust_ward, k = 5, border = 2:5)
dev.off()
```

To zoom in, right click and open picture on a new tab:

![](plotdendogram_0.png)

## Visual Hierarchical clustering Analysis

```{r}
mat_std_cluster_vars <- data.matrix(std_cluster_vars)
```

```{r}
heatmaply(mat_std_cluster_vars,
          Colv=NA,
          showticklabels = c(TRUE, FALSE),
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 5,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Nigeria LGA"
          )
```

## Mapping the clusters

```{r}
groups <- as.factor(cutree(hclust_ward, k=5))
```

```{r}
std_cluster_vars_cluster <- cbind(nga, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

```{r}
qtm(std_cluster_vars_cluster, "CLUSTER")
```

# Step 8: SKATER

```{r}
nga_0 <- nga[c(1:85, 87:774),]
```

```{r}
nga_sp <- as_Spatial(nga_0)
```

```{r}
nb_nga_sp <- poly2nb(nga_sp)
summary(nb_nga_sp)
```

```{r}
plot(nga_sp, 
     border=grey(.5))
plot(nb_nga_sp, 
     coordinates(nga_sp), 
     col="blue", 
     add=TRUE)
```

```{r}
std_cluster_vars_0 <- std_cluster_vars[c(1:85, 87:774),]
```

```{r}
lcosts <- nbcosts(nb_nga_sp, std_cluster_vars_0)
```

```{r}
weights <- nb2listw(nb_nga_sp, 
                   lcosts, 
                   style="B")
summary(weights)
```

```{r}
nga_mst <- mstree(weights)
```

```{r}
class(nga_mst)
```

```{r}
dim(nga_mst)
```

```{r}
plot(nga_sp, border=gray(.5))
plot.mst(nga_mst,
         coordinates(nga_sp),
         col="blue",
         cex.lab=0.7,
         cex.circles=0.005,
         add=TRUE)
```

```{r}
clust6 <- spdep::skater(edges = nga_mst[,1:2], 
                 data = std_cluster_vars_0, 
                 method = "euclidean", 
                 ncuts = 5)
```

```{r}
ccs6 <- clust6$groups
# ccs6
```

```{r}
plot(nga_sp, border=gray(.5))
plot(clust6, 
     coordinates(nga_sp), 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

```{r}
groups_mat <- as.matrix(clust6$groups)
nga_sf_spatialcluster <- cbind(nga_0, as.factor(groups_mat)) %>%
    rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(nga_sf_spatialcluster, "SP_CLUSTER")
```
