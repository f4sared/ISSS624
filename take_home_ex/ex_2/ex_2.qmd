---
title: "Take Home Ex 2"
editor: visual
---

# Overview

[**Files required for Take Home Ex 2:**]{.underline}

Water point attribute data (exported as CSV file):\
<https://www.waterpointdata.org/access-data/>\
Click on the button "Access WPdx+ Global Data Repository"\
Then click export button & download as CSV file

Nigeria Administrative Level 2 Boundary File:\
<https://www.geoboundaries.org/>\
Enter "Nigeria" in the name filter column.\
Download Nigeria-NGA-ADM2-2022 under the column "geoBoundaries"

Column Description can be found at:\
<https://www.waterpointdata.org/wp-content/uploads/2021/04/WPDx_Data_Standard.pdf>

# Step 1: Importing the required packages

The following packages are imported:

-   here: We use this to generate a path to the file stored at the root directory

-   sf: We use this for manipulation of simple features

-   funModeling: This is a useful package for us to do some EDA

-   tmap: We use for thematic plotting

-   tidyverse (<https://www.tidyverse.org/>)

    -   ggplot2

    -   dplyr: Used for data manipulation (e.g. mutate() )

    -   readr: We use this for reading rectangular data like csv

-   spdep: We use this for spatial related computations such as neighbors and weight matrix

-   corrplot: For plotting of the correlation object

-   heatmaply: For plotting of heatmaps and also for normalize() function

-   cluster: We use this package for performing clustering related computations

-   ClustGeo:

-   factoextra: Extract and Visualize the Results of Multivariate Data Analyses

```{r}
pacman::p_load(here,
               sf, tidyverse, spdep,
               funModeling, tmap,
               corrplot, heatmaply, cluster, ClustGeo, factoextra
               )
```

# Step 2: Shrinking the CSV file

Since the CSV file is really huge, we need to shrink it down first so we can store a copy of it on github. To do this, we will import the CSV file and then only select the rows that we are keen to keep using the select() function. The CSV will initially be imported as a tibble file format, thereafter once we select our referred columns, we also need to convert geometry coordinate before finally saving it as a rds file.

Using the here() function, we will generate a path to where the large csv file is stored:

```{r}
#| eval: true
csv_path <- here("data", "dataNigeria_2", "waterpoint", "wpdx.csv")
csv_path
```

We will use the read_csv() of the readr package to ingest the CSV file while filtering for Nigeria:

```{r}
#| eval: false
wp_nga <- read_csv(csv_path) %>% filter(`#clean_country_name` == "Nigeria")
```

We will use the select() to keep only columns of interest:

```{r}
#| eval: false
wp_nga_out <- wp_nga %>% select(7:10, 13:14, 22, 46:47, 62)
```

Using st_as_sfc() of the sf package, we derive a new "Geometry" column based on \`New Georeferenced Column\`. This is because the column is as actually holding data in textual format.

We run the code as follow:

```{r}
#| eval: false
wp_nga_out$Geometry = st_as_sfc(wp_nga_out$`New Georeferenced Column`)
```

Next using st_sf() of the sf package, we will convert the current object to sf dataframe:

```{r}
#| eval: false
wp_nga_out <- st_sf(wp_nga_out, crs=4326) 
```

Since the column \`New Georeferenced Column\` is no longer needed, we exclude it:

```{r}
#| eval: false
wp_nga_out <- wp_nga_out %>% select(1:9, 11)
```

To save the file, we need to generate a save path:

```{r}
#| eval: true
savefile_path <- here("data", "dataNigeria_2", "waterpoint", "wp_nga_v2.rds")
savefile_path
```

We then save the dataframe as a rds file:

```{r}
#| eval: false
write_rds(wp_nga_out, savefile_path)
```

# Step 3: Loading the data

## Load the water point data:

Now, using the read_rds of the readr package, we load the data as follow:\
-we rename all the columns for ease of work\
-we also replace all N/A values of the status with "Unknown"

```{r}
wp_nga <- read_rds(savefile_path) %>%
    rename(`status`=`#status_clean`) %>%
    rename(`adm1`=`#clean_adm1`) %>%
    rename(`adm2`=`#clean_adm2`) %>% 
    rename(`tech_clean`=`#water_tech_clean`) %>%
    rename(`tech_category`=`#water_tech_category`) %>%
    mutate(status = replace_na(status, "Unknown")) %>%
    mutate(tech_category = replace_na(tech_category, "Unknown")) %>% 
    mutate(tech_clean = replace_na(tech_clean, "Unknown"))
```

We will now use the st_geometry of the sf package to look at our data info:

```{r}
st_geometry(wp_nga)
```

We will check the CRS of the loaded rds file using st_crs() from sf:

```{r}
st_crs(wp_nga)
```

We will check the output:

```{r}
st_crs(wp_nga)
```

## Load the Geo Boundary data:

Once again, we need to generate the path as follow:

```{r}
boundary_path <- here("data", "dataNigeria_2", "boundary")
boundary_path
```

Using the st_read() function of the sf package, we will load the boundary file:

```{r}
nga <- st_read(dsn = boundary_path,
               layer = "geoBoundaries-NGA-ADM2",
               crs = 4326) %>% select(shapeName)
```

We will check the CRS with st_crs() from sf package:

```{r}
st_crs(nga)
```

## Handling Repeated Names

We refer to our classmates' method:\
<https://jordan-isss624-geospatial.netlify.app/posts/geo/geospatial_exercise/#data-wrangling>

Get the index of the rows that are duplicated

```{r}
bool_list <- duplicated(nga$shapeName)
duplicated_names <- nga$shapeName[bool_list]
index_rows <- which(nga$shapeName %in% duplicated_names)
index_rows
duplicated_names
```

Save the duplicated region as a new sf dataframe:

```{r}
dup_regions <- nga %>% filter(nga$shapeName %in% duplicated_names)
```

Select the rows from the waterpoint data whose adm 2 is part of the duplicated list

```{r}
dup_points <- wp_nga %>%
  filter(adm2 %in%
           c("Bassa",
             "Ifelodun",
             "Irepodun",
             "Nasarawa",
             "Obi",
             "Surulere"))
```

Perform a simple intersection between the duplicated regions and the earlier filtered waterpoints:

```{r}
dup_intersect <- st_intersects(dup_regions, dup_points)
```

Next, we will use the slice function to select rows that fall under each of the duplicated region, then we will display the column "adm1" which shows the sub name for that region.

Bassa 1:

```{r}
(dup_points %>% slice(unlist(dup_intersect[1])))$adm1 %>% head()
```

Bassa 2:

```{r}
(dup_points %>% slice(unlist(dup_intersect[2])))$adm1 %>% head()
```

Ifelodun 1:

```{r}
(dup_points %>% slice(unlist(dup_intersect[3])))$adm1 %>% head()
```

Ifelodun 2:

```{r}
(dup_points %>% slice(unlist(dup_intersect[4])))$adm1 %>% head()
```

Irepodun 1:

```{r}
(dup_points %>% slice(unlist(dup_intersect[5])))$adm1 %>% head()
```

Irepodun 2:

```{r}
(dup_points %>% slice(unlist(dup_intersect[6])))$adm1 %>% head()
```

Nasarawa 1:

```{r}
(dup_points %>% slice(unlist(dup_intersect[7])))$adm1 %>% head()
```

Nasarawa 2:

```{r}
(dup_points %>% slice(unlist(dup_intersect[8])))$adm1 %>% head()
```

Obi 1:

```{r}
(dup_points %>% slice(unlist(dup_intersect[9])))$adm1 %>% head()
```

Obi 2:

```{r}
(dup_points %>% slice(unlist(dup_intersect[10])))$adm1 %>% head()
```

Surulere 1:

```{r}
(dup_points %>% slice(unlist(dup_intersect[11])))$adm1 %>% head()
```

Surulere 2:

```{r}
(dup_points %>% slice(unlist(dup_intersect[12])))$adm1 %>% head()
```

Plot the areas that are duplicated:

```{r}
#| eval: false
tmap_mode("view")
tm_shape(dup_rows) + tm_polygons()
```

Rename the duplicated areas

```{r}
nga$shapeName[index_rows] <- c("Bassa_Kogi", "Bassa_Plateau", 
                               "Ifelodun_Kwara", "Ifelodun_Osun",
                               "Irepodun_Kwara", "Irepodun_Osun",
                               "Nasarawa_0", "Nasarawa_Nasarawa",
                               "Obi_Benue", "Obi_Nasawara",
                               "Surulere_Lagos","Surulere_Oyo")
```

Check to see if there are any repeated rows again:

```{r}
bool_list <- duplicated(nga$shapeName)
duplicated_names <- nga$shapeName[bool_list]
index_rows <- which(nga$shapeName %in% duplicated_names)
index_rows
```

We use the rm() function to remove the environment variables to reduce clutter:

```{r}
rm(dup_intersect)
rm(dup_points)
rm(dup_regions)
rm(bool_list)
rm(duplicated_names)
rm(index_rows)
```

# Step 4: Data Wrangling

## Status of water point

Using the freq() from funModeling, we will check the distribution of status of the water points:

```{r}
freq(data=wp_nga, input = 'status')
```

### Functional water point

```{r}
wpt_functional <- wp_nga %>%
  filter(status %in%
           c("Functional",
             "Functional but not in use",
             "Functional but needs repair"))
```

```{r}
#| eval: false
freq(data=wpt_functional, 
     input = 'status')
```

### Non Functional water point

```{r}
wpt_nonfunctional <- wp_nga %>%
  filter(status %in%
           c("Abandoned/Decommissioned", 
             "Abandoned",
             "Non-Functional",
             "Non functional due to dry season",
             "Non-Functional due to dry season"))
```

```{r}
#| eval: false
freq(data=wpt_nonfunctional, 
     input = 'status')
```

### Unknown

```{r}
wpt_unknown <- wp_nga %>% filter(status == "Unknown")
```

## Water point Technology

```{r}
wpt_hand_pump <- wp_nga %>% filter(tech_category == "Hand Pump")
```

```{r}
#| eval: false
freq(data=wp_nga, input = 'tech_category')
```

## Usage Capacity

```{r}
wpt_1000 <- wp_nga %>% filter(usage_capacity < 1000)
```

```{r}
#| eval: false
freq(data=wp_nga, input = 'usage_capacity')
```

## Rural waterpoints

```{r}
wpt_rural <- wp_nga %>% filter(is_urban == FALSE)
```

```{r}
#| eval: false
freq(data=wp_nga, input = 'is_urban')
```

## Point-in-Polygon

Since each of the waterpoint in wp_nga belongs to a specific region, we need to allocated them into their own respective region, we do this using the the st_intersect() function of the sf package along with the mutate() function of the dplyr package.

We perform Point in polygon as follow:

```{r}
#| eval: true
nga_wp <- nga %>%
  mutate(`total_wpt` = lengths(
    st_intersects(nga, wp_nga))) %>%
  mutate(`wpt_functional` = lengths(
    st_intersects(nga, wpt_functional))) %>%
  mutate(`wpt_non_functional` = lengths(
    st_intersects(nga, wpt_nonfunctional))) %>%
  mutate(`wpt_unknown` = lengths(
    st_intersects(nga, wpt_unknown))) %>% 
  mutate(`wpt_hand_pump` = lengths(
    st_intersects(nga, wpt_hand_pump))) %>%
  mutate(`wpt_1000` = lengths(
    st_intersects(nga, wpt_1000))) %>% 
  mutate(`wpt_rural` = lengths(
    st_intersects(nga, wpt_rural))) 
```

Calculate Percentage:

```{r}
#| eval: true
nga_wp <- nga_wp %>%
  mutate(pct_functional = `wpt_functional`/`total_wpt`) %>%
  mutate(`pct_non_functional` = `wpt_non_functional`/`total_wpt`) %>% 
  mutate(`pct_hand_pump` = `wpt_hand_pump`/`total_wpt`) %>% 
  mutate(`pct_1000` = `wpt_1000`/`total_wpt`) %>%
  mutate(`pct_rural` = `wpt_rural`/`total_wpt`)
```

Replace NaN with 0

```{r}
#| eval: true
NaN_list <- is.nan(nga_wp$pct_functional)
nga_wp$pct_functional[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_non_functional)
nga_wp$pct_non_functional[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_hand_pump)
nga_wp$pct_hand_pump[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_hand_pump)
nga_wp$pct_hand_pump[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_1000)
nga_wp$pct_1000[NaN_list] <- 0

NaN_list <- is.nan(nga_wp$pct_rural)
nga_wp$pct_rural[NaN_list] <- 0
```

```{r}
#| eval: false
qtm(nga_wp, fill = "wpt hand pump") + 
    tm_layout(legend.height = 0.4,legend.width = 0.4)
```

Once again, we now clean the environment of variables:

```{r}
rm(wpt_1000)
rm(wpt_functional)
rm(wpt_hand_pump)
rm(wpt_nonfunctional)
rm(wpt_rural)
rm(wpt_unknown)
rm(NaN_list)
rm(wp_nga)
```

## Performing Projection to 26391

We will now use st_trasnform() function to project to the Nigeria CRS of 26391:

```{r}
nga_wp <- st_transform(nga_wp, crs = 26391)
```

We will also transform the boundary file:

```{r}
nga <- st_transform(nga, crs = 26391)
```

# Step 5: EDA

```{r}
which(nga_wp$total_wpt == 0)
```

# Step 6: Correlation Analysis

We will not get the correlation variables as follow:\
(we use st_set_geometry(NULL) of sf package to exclude geometric data)

```{r}
corr_data <- nga_wp[,c(4:5, 10:14)] %>% st_set_geometry(NULL)
```

In order to perform correlation analysis, we need to get the correlation matrix using cor() from the base R package, then we can plot it using the corrplot.mixed() function from corrplot package.

Perform correlation analysis:

```{r}
#| eval: true
cluster_vars.cor = cor(corr_data)
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

From the correlation plot above, we notice that [**pct_hand_pump**]{.underline} is highly negatively correlated with [**pct_1000**]{.underline}. Because the value is not that high extremely high, we always include it in the analysis at another time.

Perform cleanup:

```{r}
rm(cluster_vars.cor)
rm(corr_data)
```

# Step 7: Hierarchy Cluster Analysis

## Extract the clustering variables

To perform the analysis, we need to extract the cluster variables without the geometric data. To do this, we we will use the function st_set_geometry(NULL) from sf package on our sf dataframe.

We run the code as follow:

```{r}
cluster_vars <- nga_wp %>%
  st_set_geometry(NULL) %>%
  select("shapeName", "wpt_functional", "wpt_non_functional", 
         "pct_functional", "pct_non_functional", 
         "pct_hand_pump", "pct_1000", "pct_rural")
head(cluster_vars,3)
```

Next, we need to change the row names using row.name() function from R base:

```{r}
row.names(cluster_vars) <- cluster_vars$"shapeName"
head(cluster_vars,3)
```

Now, we do not need the column shapeName & pct_1000 anymore, so we exclude it:

```{r}
cluster_vars <- select(cluster_vars, c(2:6, 8))
head(cluster_vars, 3)
```

## Data Standardization

From our dataframe "done_cluster_vars", we can see that the columns [**wpt_functional**]{.underline} and [**wpt_non_functional**]{.underline} have much higher ranges since they are counts instead of percentages. Thus we need to perform standardization on these columns. This is because analysis results will be biased towards clustering variables with large values. Hence the need for standardization before cluster analysis

We will therefore perform the min-max standardization using the normalize() of the heatmaply package:\
(we use summary() of base R package to check the outcome)

```{r}
std_cluster_vars <- normalize(cluster_vars)
summary(std_cluster_vars)
```

## Compute the proximity matrix

When calculating proximity matrix, there are a few options available for us:\
-Euclidean\
-Maximum\
-Manhattan / City Block Distance\
-Canberra\
-Binary\
-Minkowski

We will use Euclidean since it is the default.\
Using the dist() of the base R package, we will compute the proximity matrix:

```{r}
proxmat <- dist(std_cluster_vars, method = 'euclidean')
```

## Compute the Clustering

There are 2 forms of clustering methods:

-   Agglomerative-AGNES: This is a bottom up manner where each point starts as a cluster

-   Divise Analysis-DIANA: This is a top down approach. At each step, most heterogeneous is split.

There are a total of eight clustering algorithms for AGNES:\
-ward.D\
-ward.D2\
-single\
-complete\
-average(UPGMA)\
-mcquitty(WPGMA)\
-median(WPGMC)\
-centroid(UPGMC)

We will start off with "ward.D"

Using hclust() of the base R package, we will compute the cluster as follow:

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

We will then get an object hclust_ward which contains the detaiLED information on the structure of the cluster tree. This will allow the later plot() function to know how to plot the structure correctly.

## Plotting the Dendrogram:

To view the outcome, we save the output as a png and then load it as a picture:\
<https://stackoverflow.com/questions/11444743/how-to-adjust-sizes-of-x-axis-in-dendrogram-r>

Each time the code is run, the .png object will be updated. For better resolution, we increase the width and height of the .png file. The "cex" is adjusted such that the names of each region is just visible when we zoom in on the .png object.

Additional parameters:

-   lwd: Line width

-   main: Title

-   line: Position of title

-   cex.main: Size of title

We run the code as follow

```{r}
png("plotdendogram.png",width=12800,height=3200)
plot(hclust_ward, cex = 1.2, lwd=3.0, main="Dendrogram", line = -20, cex.main=20)
dev.off()
```

Please open the png in a new tab and then zoom in:

![](plotdendogram.png)

## Selecting the optimal clustering algorithm:

In order the measure the strength of the clustering structure, we will need to measure the agglomerative coefficient. If we compare the aforementioned coefficient across various clustering algorithms, we will be able to select the algorithm that gives us the best clustering structure.

The function agnes() functions similar to the hclust() function with the addition of a coefficient:\
<https://rdrr.io/cran/cluster/man/agnes.html>

We now use the agnes() function from the cluster package:

```{r}
agnes_result <- agnes(std_cluster_vars, method = "ward")
agnes_result$ac
```

The results are stored in the output data structure under the name agnes_result\$ac. The higher the magnitude of this value, the better the clustering structure. In this case, "ward" gets a score of 0.990276 which shows strong clustering structure.

We will need to test all 4 of the clustering algorithms to determine which is the best to apply.

First, we need to create a data structure to hold the names of the clustering algorithms:

```{r}
algorithms <- c( "average", "single", "complete", "ward")
names(algorithms) <- c( "average", "single", "complete", "ward")
algorithms
```

Next we write a simple function "test" that takes in the name of the clustering algorithm and then outputs the score of the clustering structure:

```{r}
test <- function(x) {
  agnes(std_cluster_vars, method = x)$ac
}
```

Finally, using the map_dbl() function from the purr package, we will map the name of each algorithm to the input of the functions:

```{r}
map_dbl(algorithms, test)
```

Of the clustering algorithms, we can see that "ward" methods gives us the best clustering structure. Hence we will use this for the rest of the analysis.

We perform house keeping:

```{r}
rm(cluster_vars)
rm(agnes_result)
rm(algorithms)
# rm(proxmat)
rm(test)
```

## Determine optimal clusters

In order to perform meaningful analysis, we need to select the optimal amount of cluster. Too little clusters, we won't be able to draw meaningful insights. Too many clusters, and we will also fail to draw meaning.

There are 3 methods to do this:

-   Elbow method

-   Average silhouette method

-   Gap statistic

For this, we will make use of the tool called the gap statistic:\
<https://hastie.su.domains/Papers/gap.pdf>

The gap statistic is a measure of intra-cluster variation. The larger this value, the bigger the indication that the clustering behaviour is far from the random distribution of points. As we increase the number of cluster, we want to get a highest gap statistic number possible. The minimum number of clusters that we accept is 3 cluster for meaningful analysis.

There are a few methods to determine clusters:

-   firstSEmax

-   Tibs2001SEmax

-   globalSEmax

-   firstmax

-   globalmax

Useful links:\
<https://stats.stackexchange.com/questions/95290/how-should-i-interpret-gap-statistic>\
<https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clusGap.html>

To do this, we use the clusGap() function from the cluster package:\
(for predictability, we need to set a specific random seed)

```{r}
set.seed(2022)
gap_stat <- clusGap(std_cluster_vars, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 25, 
                    B = 50)
# Print the result
print(gap_stat, method = "Tibs2001SEmax")
```

Next, we will plot the Gap Statistics as follow:

```{r}
fviz_gap_stat(gap_stat)
```

From the graph we see that the gradient of the Gap Statistic decreases greatly from the 4th cluster onwards. Therefore, we will opt to keep 5 clusters

## Interpreting the Dendrograms

```{r}
png("plotdendogram_0.png",width=12800,height=3200)
plot(hclust_ward, cex = 1.2, lwd=3.0, main="Dendrogram", line = -20, cex.main=20)
rect.hclust(hclust_ward, k = 5, border = 2:5)
dev.off()
```

To zoom in, right click and open picture on a new tab:

![](plotdendogram_0.png)

## Visual Hierarchical clustering Analysis

In order to plot the heatmap, we need to convert the dataframe into a data matrix:

```{r}
mat_std_cluster_vars <- data.matrix(std_cluster_vars)
```

Next using heatmaply() of the heatmaply package, we can plot the interactive as follow:

```{r}
heatmaply(mat_std_cluster_vars,
          Colv=NA,
          showticklabels = c(TRUE, FALSE),
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 5,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Nigeria LGA"
          )
```

## Mapping the clusters

We first use the function cutree() of R base to prune the clusters to 5:

```{r}
groups <- as.factor(cutree(hclust_ward, k=5))
```

We will create a new sf dataframe nga_final which includes the geometry and the variables:

```{r}
nga_final <- nga_wp %>%
  select("shapeName", "wpt_functional", "wpt_non_functional", 
         "pct_functional", "pct_non_functional", 
         "pct_hand_pump", "pct_1000", "pct_rural")
```

Next, we will use cbind() from base R to save the clustering group as a column:

```{r}
bind_std_cluster_vars <- cbind(nga_final, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

Here we will do a simple filtering to allow us to get the group number of a specific region

```{r}
bind_std_cluster_vars %>% filter(shapeName == "Kanke")
```

Using qtm() of the tmap, we will perform a quick plot as follow:

```{r, fig.width = 8, fig.height = 8}
tmap_mode("plot")
tm_shape(bind_std_cluster_vars) + 
  tm_fill("CLUSTER") + 
  tm_borders(alpha = 0.9) + 
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_layout(main.title = "Cluster Distribution of Nigeria",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) 
```

From the above plot, we can see that if we do clustering without consideration for geographic location, we get a cluster map that has clusters that are highly fragmented.

------------------------------------------------------------------------

Housekeeping to remove variables from environment:

```{r}
rm(gap_stat)
rm(hclust_ward)
rm(mat_std_cluster_vars)
rm(groups)
```

# Step 8: SKATER

Unconstrained clustering often will show some form of regional clustering, but if we want all members of the clusters to show spatially contiguous relationships, we need specialized algorithm such as SKATER.

A key idea here is that traditional cluster valuation measures is sub-optimal in practice because of real-world constraints. As shown from the unconstrained cluster evaluation above, our clusters turned out to be quite fragmented.

Thus, the advantage of using SKATER is that it provides a hard requirement that spatial objects in the same cluster are also geographically linked. It is a rationalization approach using graph partitioning while preserving neighborhood connectivity.

Additional Info for SKATER:\
<https://www.dshkol.com/post/spatially-constrained-clustering-and-regionalization/>

## Check for regions without neighbors:

Using the summary function, we will check if all polygons have a neighbor:

```{r}
summary(poly2nb(nga))
```

It appears that polygon from row 86 does not have a neighbor:

```{r}
poly2nb(nga)[86]
```

## Remove regions without neighbors:

We remove row 86 from boundary file:

```{r}
nga <- nga[c(1:85, 87:774),]
```

We do the same for the clustering variables:

```{r}
bind_std_cluster_vars <- bind_std_cluster_vars[c(1:85, 87:774),]
nga_wp <- nga_wp[c(1:85, 87:774),]
std_cluster_vars <- std_cluster_vars[c(1:85, 87:774),]
nga_final <- nga_final[c(1:85, 87:774),]
```

## Compute the neighbors and the cost:

### Neighbor

We will convert the boundary file to a spatial file and then feed it to the function poly2nb():

```{r}
nga_sp <- as_Spatial(nga)
nga.nb <- poly2nb(nga_sp)
summary(nga.nb)
```

We can then plot the neighbor relationship:

```{r, fig.width = 12, fig.height = 12}
plot(nga_sp, 
     border=grey(.5), main="Neighbor Map", cex.main=4)
plot(nga.nb, 
     coordinates(nga_sp), 
     col="blue", 
     add=TRUE)
```

### Edge cost and weights

Next we will use the neighbor list and the clustering variables to compute the edge cost\
using nbcosts() of spdep package:

```{r}
lcosts <- nbcosts(nga.nb, std_cluster_vars)
```

Next we need to construct the weights taking into account the edge cost and the neighbors:\
(We need to use "B" for binary here since we want the edge cost)

```{r}
nga.w <- nb2listw(nga.nb, 
                   lcosts, 
                   style="B")
summary(nga.w)
```

## Construct Minimum Spanning Tree

Now, using mstree() of the spdep package, we will construct the minimum spanning tree:

```{r}
nga.mst <- mstree(nga.w)
```

We now check the class:

```{r}
class(nga.mst)
```

We also check the dimension:

```{r}
dim(nga.mst)
```

We can check the contents of the minimum spanning tree:

```{r}
head(nga.mst)
```

Finally we can plot the minimum spanning tree as follow:\
(We will set the size of label to be small so we can see the tree)

```{r, fig.width = 12, fig.height = 12}
plot(nga_sp, border=gray(.5), main="Minimum Spanning Tree", cex.main=4)
plot.mst(nga.mst,
         coordinates(nga_sp),
         col="blue",
         cex.lab=0.1,
         cex.circles=0.005,
         add=TRUE)
```

Housekeeping

```{r}
rm(lcosts)
rm(nga.w)
rm(nga.nb)
```

## Compute Spatially constrained clusters with SKATER method

Now that we have the minimum spanning tree, we can then compute the spatially constrained cluster using the skater() package of spdep. We will be needing the minimum spanning tree computed earlier as well as the standardized cluster variables.

Note: the number of cuts is not equal to the number of clusters. For cluster 5, the number of cuts required is actually 4.

We run the code as follow:

```{r}
clust5 <- spdep::skater(edges = nga.mst[,1:2], 
                 data = std_cluster_vars, 
                 method = "euclidean", 
                 ncuts = 4)
```

```{r}
#| eval: false
ccs5 <- clust5$groups
```

Next we will plot our the lines for each individual cluster:

```{r, fig.width = 12, fig.height = 12}
plot(nga_sp, border=gray(.1), main="SKATER Clusters", cex.main=4)
plot(clust5, 
     coordinates(nga_sp), 
     cex.lab=0.1,
     groups.colors=c("red","green","blue", "brown", "black"),
     cex.circles=0.005, lwd=3.0,
     add=TRUE)
```

Lastly we can visualize our outcome with qtm() of the tmap:

```{r, fig.width = 8, fig.height = 8}
groups_mat <- as.matrix(clust5$groups)
nga_sf_spatialcluster <- cbind(nga_final, as.factor(groups_mat)) %>%
    rename(`SP_CLUSTER`=`as.factor.groups_mat.`)

tm_shape(nga_sf_spatialcluster) + 
  tm_fill("SP_CLUSTER") + 
  tm_borders(alpha = 0.9) + 
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_layout(main.title = "Cluster Distribution of Nigeria",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) 
```

After applying the SKATER method, we not see that the clusters becomes much less fragmented. as compared to earlier.

We can easily compare the difference using tmap_arrange() from the tmap package:

```{r, fig.width = 12, fig.height = 6}
tmap_mode("plot")
normal_cluster.map <- tm_shape(bind_std_cluster_vars) + 
                      tm_fill("CLUSTER") + 
                      tm_borders(alpha = 1.0) + 
                      tm_compass(type="8star", size = 2) +
                      tm_scale_bar() +
                      tm_grid(alpha =0.2) +
                      tm_layout(main.title = "Cluster Distribution of Nigeria",
                                main.title.position = "center",
                                main.title.size = 1.2,
                                legend.height = 0.45, 
                                legend.width = 0.35,
                                frame = TRUE) 

skater_cluster.map <- tm_shape(nga_sf_spatialcluster) + 
                      tm_fill("SP_CLUSTER") + 
                      tm_borders(alpha = 1.0) + 
                      tm_compass(type="8star", size = 2) +
                      tm_scale_bar() +
                      tm_grid(alpha =0.2) +
                      tm_layout(main.title = "Cluster Distribution of Nigeria",
                                main.title.position = "center",
                                main.title.size = 1.2,
                                legend.height = 0.45, 
                                legend.width = 0.35,
                                frame = TRUE) 

tmap_arrange(normal_cluster.map, skater_cluster.map,
             asp=NA, ncol=2)
```

------------------------------------------------------------------------

Housekeeping

```{r}
rm(clust5)
rm(groups_mat)
rm(normal_cluster.map)
rm(skater_cluster.map)
```

# Step 9: Spatially Constrained Method with ClustGeo

ClustGeo is a useful package that allows us to mix and find the best balance between maintaining spatial contiguity and the solution based on the variable of interest. All this is controlled by the mixing variable alpha.

There are 2 matrix D0 & D1 that is involved:\
-D0: This matrix represents the dissimilarities in the attribute/clustering variable space.\
-D1: This matrix represents the dissimilarities in the spatial domain.

Our goal is thus to find a good balance between this 2 matrix using the alpha variable.

## Traditional Ward-Like clustering with ClustGeo

ClustGeo also offers a non-spatially constrained clustering with hclustgeo() as follow:

```{r}
non_geo_cluster <- hclustgeo(proxmat)

png("plotdendogram_1.png",width=12800,height=3200)
plot(non_geo_cluster, cex = 1.2, lwd=3.0, main="Dendrogram", 
     line = -20, cex.main=20)
rect.hclust(non_geo_cluster, k = 5, border = 2:5)
dev.off()
```

![](plotdendogram_1.png)

Housekeeping:

```{r}
rm(non_geo_cluster) 
```

## Preparing Spatially Constrained Hierarchical Clustering

To perform the clustering, we need 3 ingredients:\
-Distance Matrix: Represents the spatial domain\
-Proximity Matrix: Represents the variable domain\
-Mixing variable Alpha: This is required for denoting the degree of mix

### Distance Matrix

We will use st_distance() of the sf package to compute the distance matrix:

```{r}
dist <- st_distance(nga_final, nga_final)
distmat <- as.dist(dist)
```

### Proximity Matrix

Same as earlier, we will use dist() of the base R package to compute the variable proximity matrix:

```{r}
proxmat <- dist(std_cluster_vars, method = 'euclidean')
```

### Mixing Variable Alpha

Using the above 2 ingredients, we can now get the third ingredient.

We use the function choicealpha() from the ClustGeo package as follow:

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, 
                  graph = TRUE)
```

From the graph plotted, we observe that the optimal Qnorm value is at 0.5, we will now use this as the mixing variable.

## Spatially Constrained Hierarchical Clustering

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.5)
```

```{r}
groups <- as.factor(cutree(clustG, k=5))
```

```{r}
nga_sf_Gcluster <- cbind(nga_final, as.matrix(groups)) %>%
                  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
qtm(nga_sf_Gcluster, "CLUSTER")
```

# Step 10: Interpreting our results

```{r}
ggplot(data = nga_sf_Gcluster,
       aes(x = CLUSTER, y = pct_rural)) +
  geom_boxplot()
```
